mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty1Mix0.csv", delim=",")
# Define the model with tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
set.seed(123) # For reproducibility
cv_folds <- vfold_cv(dataTrain, v = 5)
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)),  # Adjust range as necessary
mixture(range = c(0, 1)),   # Mixture from Ridge (0) to Lasso (1)
levels = 10                 # Number of levels to try for each parameter
)
tuned_results <- preg_wf %>%
tune_grid(
resamples = cv_folds,
grid = penalty_grid,
metrics = metric_set(rmse) # Use Root Mean Squared Error as the metric
)
best_params <- select_best(tuned_results, "rmse")
#-----------------Penalized Regression------------------------------#
# Define the model with tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
set.seed(123) # For reproducibility
cv_folds <- vfold_cv(dataTrain, v = 5)
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)),  # Adjust range as necessary
mixture(range = c(0, 1)),   # Mixture from Ridge (0) to Lasso (1)
levels = 10                 # Number of levels to try for each parameter
)
tuned_results <- preg_wf %>%
tune_grid(
resamples = cv_folds,
grid = penalty_grid,
metrics = metric_set(rmse) # Use Root Mean Squared Error as the metric
)
best_params <- select_best(tuned_results, "rmse")
# Define the model with tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
set.seed(123) # For reproducibility
cv_folds <- vfold_cv(dataTrain, v = 5)
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)),  # Adjust range as necessary
mixture(range = c(0, 1)),   # Mixture from Ridge (0) to Lasso (1)
levels = 10                 # Number of levels to try for each parameter
)
tuned_results <- preg_wf %>%
tune_grid(
resamples = cv_folds,
grid = penalty_grid,
metrics = metric_set(rmse) # Use Root Mean Squared Error as the metric
)
best_params <- select_best(tuned_results)
print(best_params)
final_preg_wf <- preg_wf %>%
finalize_workflow(best_params)
final_fit <- final_preg_wf %>%
fit(data = dataTrain)
bike_predictions <- predict(final_fit, new_data = dataTest)
kaggle_submission <- bike_predictions %>%
bind_cols(dataTest) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(x = kaggle_submission, file = "./OptimalPenaltyMix.csv", delim = ",")
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
predict(preg_wf, new_data=dataTest)
kaggle_submission <- bike_predictions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty1Mix0.csv", delim=",")
vroom_write(x=kaggle_submission, file="./Penalty0.01Mix1.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
dataTrain <- vroom("train.csv")
dataTest <- vroom("test.csv")
dataTrain <- dataTrain %>%
select(-casual, -registered) %>%
mutate(count = log(count))
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
predict(preg_wf, new_data=dataTest)
kaggle_submission <- bike_predictions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
dataTrain <- vroom("train.csv")
dataTest <- vroom("test.csv")
dataTrain <- dataTrain %>%
select(-casual, -registered) %>%
mutate(count = log(count))
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
predict(preg_wf, new_data=dataTest)
kaggle_submission <- bike_predictions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
bike_predictions <- predict(bike_workflow, new_data = dataTest)
bike_predictions <- exp(bike_predictions)
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
dataTrain <- vroom("train.csv")
dataTest <- vroom("test.csv")
dataTrain <- dataTrain %>%
select(-casual, -registered) %>%
mutate(count = log(count))
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bike_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_linear_model) %>%
fit(data = dataTrain)
bike_predictions <- predict(bike_workflow, new_data = dataTest)
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
dataTrain <- vroom("train.csv")
dataTest <- vroom("test.csv")
dataTrain <- dataTrain %>%
select(-casual, -registered) %>%
mutate(count = log(count))
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
predict(preg_wf, new_data=dataTest)
kaggle_submission <- bike_predictions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
preg_preditions <- predict(preg_wf, new_data=dataTest)
kaggle_submission <- preg_predictions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty0.01Mix1.csv", delim=",")
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Penalized regression model
preg_model <- linear_reg(penalty=0, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty0.01Mix1.csv", delim=",")
preg_model <- linear_reg(penalty=1, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty1Mix1.csv", delim=",")
preg_model <- linear_reg(penalty=1, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
preg_preditions <- exp(preg_preditions)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty1Mix1.csv", delim=",")
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=1) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
preg_preditions <- exp(preg_preditions)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty1Mix1.csv", delim=",")
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=0.5) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
preg_preditions <- exp(preg_preditions)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty0.01Mix0.5.csv", delim=",")
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=.99) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
preg_preditions <- exp(preg_preditions)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty0.01Mix0.5.csv", delim=",")
## Penalized regression model
preg_model <- linear_reg(penalty=0.001, mixture=.999) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
preg_preditions <- exp(preg_preditions)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty0.001Mix0.999.csv", delim=",")
# Define the model with penalty and mixture as tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Create the workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
# Define a grid of penalty and mixture values
penalty_grid <- grid_regular(
penalty(range = c(-4, 0)),  # This corresponds to values between 0.0001 and 1
mixture(range = c(0, 1)),   # This corresponds to mixture values between 0 and 1
levels = 10  # Number of levels to try
)
# Set up cross-validation
set.seed(123)
cv_folds <- vfold_cv(dataTrain, v = 5)
# Tune the model
preg_tuned <- preg_wf %>%
tune_grid(resamples = cv_folds,
grid = penalty_grid,
control = control_grid(save_pred = TRUE))
# Select best tuning parameters based on RMSE
best_params <- preg_tuned %>%
select_best("rmse")
# Load libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
# Load data
dataTrain <- vroom("train.csv")
dataTest <- vroom("test.csv")
# Preprocess the data and create recipe
dataTrain <- dataTrain %>%
select(-casual, -registered) %>%
mutate(count = log(count))
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features = c("hour")) %>%
step_date(datetime, features = c("month")) %>%
step_cut(datetime_hour, breaks = c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv()  # Remove zero-variance predictors
# Define the model with penalty and mixture as tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Create the workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
# Define a grid of penalty and mixture values
penalty_grid <- grid_regular(
penalty(range = c(-4, 0)),  # This corresponds to values between 0.0001 and 1
mixture(range = c(0, 1)),   # This corresponds to mixture values between 0 and 1
levels = 10  # Number of levels to try
)
# Set up cross-validation
set.seed(123)
cv_folds <- vfold_cv(dataTrain, v = 5)
# Define metrics for evaluation
metrics <- metric_set(rmse, rsq)
# Tune the model and save predictions
preg_tuned <- preg_wf %>%
tune_grid(resamples = cv_folds,
grid = penalty_grid,
metrics = metrics,  # Add metrics here
control = control_grid(save_pred = TRUE))
# Select the best parameters based on RMSE
best_params <- preg_tuned %>%
select_best("rmse")
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
dataTrain <- vroom("train.csv")
dataTest <- vroom("test.csv")
dataTrain <- dataTrain %>%
select(-casual, -registered) %>%
mutate(count = log(count))
my_recipe <- recipe(count ~ ., data = dataTrain) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime, features=c("hour")) %>%
step_date(datetime, features=c("month")) %>%
step_cut(datetime_hour, breaks=c(7, 15, 24)) %>%
step_rm(datetime, atemp, season) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Penalized regression model
preg_model <- linear_reg(penalty=0.01, mixture=.99) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=dataTrain)
preg_preditions <- predict(preg_wf, new_data=dataTest)
preg_preditions <- exp(preg_preditions)
kaggle_submission <- preg_preditions %>%
bind_cols(., dataTest) |>
select(datetime, .pred) |>
rename(count=.pred) |>
mutate(count=pmax(0, count)) |>
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./Penalty0.01Mix0.99.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(skimr)
library(patchwork)
library(DataExplorer)
library(recipes)
library(dplyr)
library(poissonreg)
